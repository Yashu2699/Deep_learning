{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1afce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets.mnist import load_data\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import Activation, LeakyReLU, BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84fd9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_discriminator(im_shape=(28,28,1)):\n",
    "  init = RandomNormal(stddev=0.02)\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init,\n",
    "                   input_shape=im_shape))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "  model.add(Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(1, activation='linear', kernel_initializer=init))\n",
    "\n",
    "  model.compile(loss='mse', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8119d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_generator(latent_dim):\n",
    "  init = RandomNormal(stddev=0.02)\n",
    "  model = Sequential()\n",
    "  n_nodes = 256*7*7\n",
    "  model.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Reshape((7,7,256)))\n",
    "\n",
    "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "\n",
    "  model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "\n",
    "  model.add(Conv2D(1, (7,7), padding='same', kernel_initializer=init))\n",
    "  model.add(Activation('tanh'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fce88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_gan(generator, discriminator):\n",
    "  discriminator.trainable = False\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(generator)\n",
    "  model.add(discriminator)\n",
    "\n",
    "  model.compile(loss='mse', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4055af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples():\n",
    "  (trainX, _), (_, _) = load_data()\n",
    "  X = np.expand_dims(trainX, axis=-1)\n",
    "  X = X.astype('float32')\n",
    "  X = (X - 127.5) / 127.5\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d87308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "  idx = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "  X = dataset[idx]\n",
    "  y = np.ones((n_samples, 1))\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b336873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "  x_input = np.random.rand(latent_dim * n_samples)\n",
    "  x_input = x_input.reshape(n_samples, latent_dim)\n",
    "  return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc1639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "  x_input = generate_latent_points(latent_dim, n_samples)\n",
    "  X = generator.predict(x_input)\n",
    "  y = np.zeros((n_samples, 1))\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19922285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(epoch, g_model, latent_dim, n_samples=100):\n",
    "  X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "  X = X + 1 / 2\n",
    "  for i in range(n_samples):\n",
    "    plt.subplot(10, 10, 1+i)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "  filename1 = 'generated_plot_%06d.png' %(epoch+1)\n",
    "  plt.savefig(filename1)\n",
    "  plt.close()\n",
    "\n",
    "  filename2 = 'model_%06d.h5' % (epoch+1)\n",
    "  g_model.save(filename2)\n",
    "  print('saved %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5327dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "  plt.plot(d1_hist, label='d1loss')\n",
    "  plt.plot(d2_hist, label='d2loss')\n",
    "  plt.plot(g_hist, label='gan loss')\n",
    "  plt.legend()\n",
    "  filename = 'plot_loss.png'\n",
    "  plt.savefig(filename)\n",
    "  plt.close()\n",
    "  print('saved %s' % (filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166a4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=20, n_batch=64):\n",
    "  bat_per_epoch = int(dataset.shape[0] / n_batch)\n",
    "  n_steps = bat_per_epoch * n_epochs\n",
    "  half_batch = int(n_batch/2)\n",
    "  d1_hist, d2_hist, g_hist = list(), list(), list()\n",
    "\n",
    "  for i in range(n_steps):\n",
    "    X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "    X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\n",
    "    d_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "    d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "    g_input = generate_latent_points(latent_dim, n_batch)\n",
    "    y_real_2 = np.ones((n_batch, 1))\n",
    "    g_loss = gan_model.train_on_batch(g_input, y_real_2)\n",
    "\n",
    "    d1_hist.append(d_loss1)\n",
    "    d2_hist.append(d_loss2)\n",
    "    g_hist.append(g_loss)\n",
    "\n",
    "    if (i+1) % (bat_per_epoch * 1) == 0:\n",
    "      summarize_performance(i, g_model, latent_dim)\n",
    "\n",
    "    plot_history(d1_hist, d2_hist, g_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "latent_dim = 100\n",
    "discriminator = def_discriminator()\n",
    "generator = def_generator(latent_dim)\n",
    "gan_model = def_gan(generator, discriminator)\n",
    "dataset = load_real_samples()\n",
    "print(dataset.shape)\n",
    "start_time = time.time()\n",
    "train(generator, discriminator, gan_model, dataset, latent_dim)\n",
    "end_time = time.time()\n",
    "print('Total Time', (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e70ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32998777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(examples, n):\n",
    "    for i in range(n*n):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_018740.h5')\n",
    "latent_points = generate_latent_points(100, 100)\n",
    "X = model.predict(latent_points)\n",
    "plot_generated_images(X, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
